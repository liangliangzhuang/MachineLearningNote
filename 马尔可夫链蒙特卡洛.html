<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>12 马尔可夫链蒙特卡洛 | 机器学习白板系列</title>
  <meta name="description" content="这是用R的bookdown功能制作中文图书的模板，输出格式为bookdown::gitbook和bookdown::pdf_book." />
  <meta name="generator" content="bookdown 0.26 and GitBook 2.6.7" />

  <meta property="og:title" content="12 马尔可夫链蒙特卡洛 | 机器学习白板系列" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="这是用R的bookdown功能制作中文图书的模板，输出格式为bookdown::gitbook和bookdown::pdf_book." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="12 马尔可夫链蒙特卡洛 | 机器学习白板系列" />
  
  <meta name="twitter:description" content="这是用R的bookdown功能制作中文图书的模板，输出格式为bookdown::gitbook和bookdown::pdf_book." />
  

<meta name="author" content="庄闪闪" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="变分推断.html"/>
<link rel="next" href="线性动态系统.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  jax: ["input/TeX","output/SVG"],
  extensions: ["tex2jax.js","MathMenu.js","MathZoom.js"],
  TeX: {
    extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
  }
});
</script>
<script type="text/javascript"
   src="../../../MathJax/MathJax.js">
</script>




</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>简介</a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#频率派的观点"><i class="fa fa-check"></i><b>1.1</b> 频率派的观点</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#贝叶斯派的观点"><i class="fa fa-check"></i><b>1.2</b> 贝叶斯派的观点</a></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#小结"><i class="fa fa-check"></i><b>1.3</b> 小结</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="mathbasics.html"><a href="mathbasics.html"><i class="fa fa-check"></i><b>2</b> MathBasics</a>
<ul>
<li class="chapter" data-level="2.1" data-path="mathbasics.html"><a href="mathbasics.html#高斯分布"><i class="fa fa-check"></i><b>2.1</b> 高斯分布</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="mathbasics.html"><a href="mathbasics.html#一维情况-mle"><i class="fa fa-check"></i><b>2.1.1</b> 一维情况 MLE</a></li>
<li class="chapter" data-level="2.1.2" data-path="mathbasics.html"><a href="mathbasics.html#多维情况"><i class="fa fa-check"></i><b>2.1.2</b> 多维情况</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="线性回归.html"><a href="线性回归.html"><i class="fa fa-check"></i><b>3</b> 线性回归</a>
<ul>
<li class="chapter" data-level="3.1" data-path="线性回归.html"><a href="线性回归.html#最小二乘法"><i class="fa fa-check"></i><b>3.1</b> 最小二乘法</a></li>
<li class="chapter" data-level="3.2" data-path="线性回归.html"><a href="线性回归.html#噪声为高斯分布的-mle"><i class="fa fa-check"></i><b>3.2</b> 噪声为高斯分布的 MLE</a></li>
<li class="chapter" data-level="3.3" data-path="线性回归.html"><a href="线性回归.html#权重先验也为高斯分布的-map"><i class="fa fa-check"></i><b>3.3</b> 权重先验也为高斯分布的 MAP</a></li>
<li class="chapter" data-level="3.4" data-path="线性回归.html"><a href="线性回归.html#正则化"><i class="fa fa-check"></i><b>3.4</b> 正则化</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="线性回归.html"><a href="线性回归.html#l1-lasso"><i class="fa fa-check"></i><b>3.4.1</b> L1 Lasso</a></li>
<li class="chapter" data-level="3.4.2" data-path="线性回归.html"><a href="线性回归.html#l2-ridge"><i class="fa fa-check"></i><b>3.4.2</b> L2 Ridge</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="线性回归.html"><a href="线性回归.html#小结-1"><i class="fa fa-check"></i><b>3.5</b> 小结</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="线性分类.html"><a href="线性分类.html"><i class="fa fa-check"></i><b>4</b> 线性分类</a>
<ul>
<li class="chapter" data-level="4.1" data-path="线性分类.html"><a href="线性分类.html#两分类-硬分类-感知机算法"><i class="fa fa-check"></i><b>4.1</b> 两分类-硬分类-感知机算法</a></li>
<li class="chapter" data-level="4.2" data-path="线性分类.html"><a href="线性分类.html#两分类-硬分类-线性判别分析-lda"><i class="fa fa-check"></i><b>4.2</b> 两分类-硬分类-线性判别分析 LDA</a></li>
<li class="chapter" data-level="4.3" data-path="线性分类.html"><a href="线性分类.html#两分类-软分类-概率判别模型-logistic-回归"><i class="fa fa-check"></i><b>4.3</b> 两分类-软分类-概率判别模型-Logistic 回归</a></li>
<li class="chapter" data-level="4.4" data-path="线性分类.html"><a href="线性分类.html#两分类-软分类-概率生成模型-高斯判别分析-gda"><i class="fa fa-check"></i><b>4.4</b> 两分类-软分类-概率生成模型-高斯判别分析 GDA</a></li>
<li class="chapter" data-level="4.5" data-path="线性分类.html"><a href="线性分类.html#两分类-软分类-概率生成模型-朴素贝叶斯"><i class="fa fa-check"></i><b>4.5</b> 两分类-软分类-概率生成模型-朴素贝叶斯</a></li>
<li class="chapter" data-level="4.6" data-path="线性分类.html"><a href="线性分类.html#小结-2"><i class="fa fa-check"></i><b>4.6</b> 小结</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="降维.html"><a href="降维.html"><i class="fa fa-check"></i><b>5</b> 降维</a>
<ul>
<li class="chapter" data-level="5.1" data-path="降维.html"><a href="降维.html#线性降维-主成分分析-pca"><i class="fa fa-check"></i><b>5.1</b> 线性降维-主成分分析 PCA</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="降维.html"><a href="降维.html#损失函数"><i class="fa fa-check"></i><b>5.1.1</b> 损失函数</a></li>
<li class="chapter" data-level="5.1.2" data-path="降维.html"><a href="降维.html#svd-与-pcoa"><i class="fa fa-check"></i><b>5.1.2</b> SVD 与 PCoA</a></li>
<li class="chapter" data-level="5.1.3" data-path="降维.html"><a href="降维.html#p-pca"><i class="fa fa-check"></i><b>5.1.3</b> p-PCA</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="降维.html"><a href="降维.html#小结-3"><i class="fa fa-check"></i><b>5.2</b> 小结</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="支撑向量机.html"><a href="支撑向量机.html"><i class="fa fa-check"></i><b>6</b> 支撑向量机</a>
<ul>
<li class="chapter" data-level="6.1" data-path="支撑向量机.html"><a href="支撑向量机.html#约束优化问题"><i class="fa fa-check"></i><b>6.1</b> 约束优化问题</a></li>
<li class="chapter" data-level="6.2" data-path="支撑向量机.html"><a href="支撑向量机.html#hard-margin-svm"><i class="fa fa-check"></i><b>6.2</b> Hard-margin SVM</a></li>
<li class="chapter" data-level="6.3" data-path="支撑向量机.html"><a href="支撑向量机.html#soft-margin-svm"><i class="fa fa-check"></i><b>6.3</b> Soft-margin SVM</a></li>
<li class="chapter" data-level="6.4" data-path="支撑向量机.html"><a href="支撑向量机.html#kernel-method"><i class="fa fa-check"></i><b>6.4</b> Kernel Method</a></li>
<li class="chapter" data-level="6.5" data-path="支撑向量机.html"><a href="支撑向量机.html#小结-4"><i class="fa fa-check"></i><b>6.5</b> 小结</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="指数族分布.html"><a href="指数族分布.html"><i class="fa fa-check"></i><b>7</b> 指数族分布</a>
<ul>
<li class="chapter" data-level="7.1" data-path="指数族分布.html"><a href="指数族分布.html#一维高斯分布"><i class="fa fa-check"></i><b>7.1</b> 一维高斯分布</a></li>
<li class="chapter" data-level="7.2" data-path="指数族分布.html"><a href="指数族分布.html#充分统计量和对数配分函数的关系"><i class="fa fa-check"></i><b>7.2</b> 充分统计量和对数配分函数的关系</a></li>
<li class="chapter" data-level="7.3" data-path="指数族分布.html"><a href="指数族分布.html#充分统计量和极大似然估计"><i class="fa fa-check"></i><b>7.3</b> 充分统计量和极大似然估计</a></li>
<li class="chapter" data-level="7.4" data-path="指数族分布.html"><a href="指数族分布.html#最大熵"><i class="fa fa-check"></i><b>7.4</b> 最大熵</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="概率图模型.html"><a href="概率图模型.html"><i class="fa fa-check"></i><b>8</b> 概率图模型</a>
<ul>
<li class="chapter" data-level="8.1" data-path="概率图模型.html"><a href="概率图模型.html#有向图-贝叶斯网络"><i class="fa fa-check"></i><b>8.1</b> 有向图-贝叶斯网络</a></li>
<li class="chapter" data-level="8.2" data-path="概率图模型.html"><a href="概率图模型.html#无向图-马尔可夫网络马尔可夫随机场"><i class="fa fa-check"></i><b>8.2</b> 无向图-马尔可夫网络（马尔可夫随机场）</a></li>
<li class="chapter" data-level="8.3" data-path="概率图模型.html"><a href="概率图模型.html#两种图的转换-道德图"><i class="fa fa-check"></i><b>8.3</b> 两种图的转换-道德图</a></li>
<li class="chapter" data-level="8.4" data-path="概率图模型.html"><a href="概率图模型.html#更精细的分解-因子图"><i class="fa fa-check"></i><b>8.4</b> 更精细的分解-因子图</a></li>
<li class="chapter" data-level="8.5" data-path="概率图模型.html"><a href="概率图模型.html#推断"><i class="fa fa-check"></i><b>8.5</b> 推断</a>
<ul>
<li class="chapter" data-level="8.5.1" data-path="概率图模型.html"><a href="概率图模型.html#推断-变量消除ve"><i class="fa fa-check"></i><b>8.5.1</b> 推断-变量消除（VE）</a></li>
<li class="chapter" data-level="8.5.2" data-path="概率图模型.html"><a href="概率图模型.html#推断-信念传播bp"><i class="fa fa-check"></i><b>8.5.2</b> 推断-信念传播（BP）</a></li>
<li class="chapter" data-level="8.5.3" data-path="概率图模型.html"><a href="概率图模型.html#推断-max-product-算法"><i class="fa fa-check"></i><b>8.5.3</b> 推断-Max-Product 算法</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="期望最大.html"><a href="期望最大.html"><i class="fa fa-check"></i><b>9</b> 期望最大</a>
<ul>
<li class="chapter" data-level="9.1" data-path="期望最大.html"><a href="期望最大.html#广义-em"><i class="fa fa-check"></i><b>9.1</b> 广义 EM</a></li>
<li class="chapter" data-level="9.2" data-path="期望最大.html"><a href="期望最大.html#em-的推广"><i class="fa fa-check"></i><b>9.2</b> EM 的推广</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="高斯混合模型.html"><a href="高斯混合模型.html"><i class="fa fa-check"></i><b>10</b> 高斯混合模型</a>
<ul>
<li class="chapter" data-level="10.1" data-path="高斯混合模型.html"><a href="高斯混合模型.html#极大似然估计"><i class="fa fa-check"></i><b>10.1</b> 极大似然估计</a></li>
<li class="chapter" data-level="10.2" data-path="高斯混合模型.html"><a href="高斯混合模型.html#em-求解-gmm"><i class="fa fa-check"></i><b>10.2</b> EM 求解 GMM</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="变分推断.html"><a href="变分推断.html"><i class="fa fa-check"></i><b>11</b> 变分推断</a>
<ul>
<li class="chapter" data-level="11.1" data-path="变分推断.html"><a href="变分推断.html#基于平均场假设的变分推断"><i class="fa fa-check"></i><b>11.1</b> 基于平均场假设的变分推断</a></li>
<li class="chapter" data-level="11.2" data-path="变分推断.html"><a href="变分推断.html#sgvi"><i class="fa fa-check"></i><b>11.2</b> SGVI</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="马尔可夫链蒙特卡洛.html"><a href="马尔可夫链蒙特卡洛.html"><i class="fa fa-check"></i><b>12</b> 马尔可夫链蒙特卡洛</a>
<ul>
<li class="chapter" data-level="12.1" data-path="马尔可夫链蒙特卡洛.html"><a href="马尔可夫链蒙特卡洛.html#蒙特卡洛方法"><i class="fa fa-check"></i><b>12.1</b> 蒙特卡洛方法</a></li>
<li class="chapter" data-level="12.2" data-path="马尔可夫链蒙特卡洛.html"><a href="马尔可夫链蒙特卡洛.html#mcmc"><i class="fa fa-check"></i><b>12.2</b> MCMC</a></li>
<li class="chapter" data-level="12.3" data-path="马尔可夫链蒙特卡洛.html"><a href="马尔可夫链蒙特卡洛.html#平稳分布"><i class="fa fa-check"></i><b>12.3</b> 平稳分布</a></li>
<li class="chapter" data-level="12.4" data-path="马尔可夫链蒙特卡洛.html"><a href="马尔可夫链蒙特卡洛.html#隐马尔可夫模型"><i class="fa fa-check"></i><b>12.4</b> 隐马尔可夫模型</a></li>
<li class="chapter" data-level="12.5" data-path="马尔可夫链蒙特卡洛.html"><a href="马尔可夫链蒙特卡洛.html#hmm"><i class="fa fa-check"></i><b>12.5</b> HMM</a>
<ul>
<li class="chapter" data-level="12.5.1" data-path="马尔可夫链蒙特卡洛.html"><a href="马尔可夫链蒙特卡洛.html#evaluation"><i class="fa fa-check"></i><b>12.5.1</b> Evaluation</a></li>
<li class="chapter" data-level="12.5.2" data-path="马尔可夫链蒙特卡洛.html"><a href="马尔可夫链蒙特卡洛.html#learning"><i class="fa fa-check"></i><b>12.5.2</b> Learning</a></li>
<li class="chapter" data-level="12.5.3" data-path="马尔可夫链蒙特卡洛.html"><a href="马尔可夫链蒙特卡洛.html#decoding"><i class="fa fa-check"></i><b>12.5.3</b> Decoding</a></li>
</ul></li>
<li class="chapter" data-level="12.6" data-path="马尔可夫链蒙特卡洛.html"><a href="马尔可夫链蒙特卡洛.html#小结-5"><i class="fa fa-check"></i><b>12.6</b> 小结</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="线性动态系统.html"><a href="线性动态系统.html"><i class="fa fa-check"></i><b>13</b> 线性动态系统</a></li>
<li class="chapter" data-level="14" data-path="粒子滤波.html"><a href="粒子滤波.html"><i class="fa fa-check"></i><b>14</b> 粒子滤波</a>
<ul>
<li class="chapter" data-level="14.1" data-path="粒子滤波.html"><a href="粒子滤波.html#sis"><i class="fa fa-check"></i><b>14.1</b> SIS</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="条件随机场.html"><a href="条件随机场.html"><i class="fa fa-check"></i><b>15</b> 条件随机场</a>
<ul>
<li class="chapter" data-level="15.1" data-path="条件随机场.html"><a href="条件随机场.html#crf-的-pdf"><i class="fa fa-check"></i><b>15.1</b> CRF 的 PDF</a></li>
<li class="chapter" data-level="15.2" data-path="条件随机场.html"><a href="条件随机场.html#边缘概率"><i class="fa fa-check"></i><b>15.2</b> 边缘概率</a></li>
<li class="chapter" data-level="15.3" data-path="条件随机场.html"><a href="条件随机场.html#参数估计"><i class="fa fa-check"></i><b>15.3</b> 参数估计</a></li>
<li class="chapter" data-level="15.4" data-path="条件随机场.html"><a href="条件随机场.html#译码"><i class="fa fa-check"></i><b>15.4</b> 译码</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="高斯网络.html"><a href="高斯网络.html"><i class="fa fa-check"></i><b>16</b> 高斯网络</a>
<ul>
<li class="chapter" data-level="16.1" data-path="高斯网络.html"><a href="高斯网络.html#高斯贝叶斯网络-gbn"><i class="fa fa-check"></i><b>16.1</b> 高斯贝叶斯网络 GBN</a></li>
<li class="chapter" data-level="16.2" data-path="高斯网络.html"><a href="高斯网络.html#高斯马尔可夫网络-gmn"><i class="fa fa-check"></i><b>16.2</b> 高斯马尔可夫网络 GMN</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="贝叶斯线性回归.html"><a href="贝叶斯线性回归.html"><i class="fa fa-check"></i><b>17</b> 贝叶斯线性回归</a>
<ul>
<li class="chapter" data-level="17.1" data-path="贝叶斯线性回归.html"><a href="贝叶斯线性回归.html#推断-1"><i class="fa fa-check"></i><b>17.1</b> 推断</a></li>
<li class="chapter" data-level="17.2" data-path="贝叶斯线性回归.html"><a href="贝叶斯线性回归.html#预测"><i class="fa fa-check"></i><b>17.2</b> 预测</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="高斯过程回归.html"><a href="高斯过程回归.html"><i class="fa fa-check"></i><b>18</b> 高斯过程回归</a>
<ul>
<li class="chapter" data-level="18.1" data-path="高斯过程回归.html"><a href="高斯过程回归.html#核贝叶斯线性回归"><i class="fa fa-check"></i><b>18.1</b> 核贝叶斯线性回归</a></li>
<li class="chapter" data-level="18.2" data-path="高斯过程回归.html"><a href="高斯过程回归.html#函数空间的观点"><i class="fa fa-check"></i><b>18.2</b> 函数空间的观点</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="受限玻尔兹曼机.html"><a href="受限玻尔兹曼机.html"><i class="fa fa-check"></i><b>19</b> 受限玻尔兹曼机</a>
<ul>
<li class="chapter" data-level="19.1" data-path="受限玻尔兹曼机.html"><a href="受限玻尔兹曼机.html#推断-2"><i class="fa fa-check"></i><b>19.1</b> 推断</a>
<ul>
<li class="chapter" data-level="19.1.1" data-path="受限玻尔兹曼机.html"><a href="受限玻尔兹曼机.html#phv"><i class="fa fa-check"></i><b>19.1.1</b> <span class="math inline">\(p(h|v)\)</span></a></li>
<li class="chapter" data-level="19.1.2" data-path="受限玻尔兹曼机.html"><a href="受限玻尔兹曼机.html#pv"><i class="fa fa-check"></i><b>19.1.2</b> <span class="math inline">\(p(v)\)</span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="20" data-path="谱聚类.html"><a href="谱聚类.html"><i class="fa fa-check"></i><b>20</b> 谱聚类</a></li>
<li class="chapter" data-level="21" data-path="前馈神经网络.html"><a href="前馈神经网络.html"><i class="fa fa-check"></i><b>21</b> 前馈神经网络</a>
<ul>
<li class="chapter" data-level="21.1" data-path="前馈神经网络.html"><a href="前馈神经网络.html#from-pla-to-dl"><i class="fa fa-check"></i><b>21.1</b> From PLA to DL</a></li>
<li class="chapter" data-level="21.2" data-path="前馈神经网络.html"><a href="前馈神经网络.html#非线性问题"><i class="fa fa-check"></i><b>21.2</b> 非线性问题</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="配分函数.html"><a href="配分函数.html"><i class="fa fa-check"></i><b>22</b> 配分函数</a>
<ul>
<li class="chapter" data-level="22.1" data-path="配分函数.html"><a href="配分函数.html#包含配分函数的-mle"><i class="fa fa-check"></i><b>22.1</b> 包含配分函数的 MLE</a></li>
<li class="chapter" data-level="22.2" data-path="配分函数.html"><a href="配分函数.html#对比散度-cd-learning"><i class="fa fa-check"></i><b>22.2</b> 对比散度-CD Learning</a></li>
<li class="chapter" data-level="22.3" data-path="配分函数.html"><a href="配分函数.html#rbm-的学习问题"><i class="fa fa-check"></i><b>22.3</b> RBM 的学习问题</a></li>
</ul></li>
<li class="chapter" data-level="23" data-path="近似推断.html"><a href="近似推断.html"><i class="fa fa-check"></i><b>23</b> 近似推断</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">机器学习白板系列</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="马尔可夫链蒙特卡洛" class="section level1 hasAnchor" number="12">
<h1><span class="header-section-number">12</span> 马尔可夫链蒙特卡洛<a href="马尔可夫链蒙特卡洛.html#马尔可夫链蒙特卡洛" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>MCMC 是一种随机的近似推断，其核心就是基于采样的随机近似方法蒙特卡洛方法。对于采样任务来说，有下面一些常用的场景：</p>
<ol style="list-style-type: decimal">
<li>采样作为任务，用于生成新的样本</li>
<li>求和/求积分</li>
</ol>
<p>采样结束后，我们需要评价采样出来的样本点是不是好的样本集：</p>
<ol style="list-style-type: decimal">
<li>样本趋向于高概率的区域</li>
<li>样本之间必须独立</li>
</ol>
<p>具体采样中，采样是一个困难的过程：</p>
<ol style="list-style-type: decimal">
<li>无法采样得到归一化因子，即无法直接对概率 $ p(x)=(x)$ 采样，常常需要对 CDF 采样，但复杂的情况不行</li>
<li>如果归一化因子可以求得，但是对高维数据依然不能均匀采样（维度灾难），这是由于对 <span class="math inline">\(p\)</span> 维空间，总的状态空间是 <span class="math inline">\(K^p\)</span> 这么大，于是在这种情况下，直接采样也不行</li>
</ol>
<p>因此需要借助其他手段，如蒙特卡洛方法中的拒绝采样，重要性采样和 MCMC。</p>
<div id="蒙特卡洛方法" class="section level2 hasAnchor" number="12.1">
<h2><span class="header-section-number">12.1</span> 蒙特卡洛方法<a href="马尔可夫链蒙特卡洛.html#蒙特卡洛方法" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>蒙特卡洛方法旨在求得复杂概率分布下的期望值：<span class="math inline">\(\mathbb{E}_{z|x}[f(z)]=\int p(z|x)f(z)dz\simeq\frac{1}{N}\sum\limits_{i=1}^Nf(z_i)\)</span>，也就是说，从概率分布中取 <span class="math inline">\(N\)</span> 个点，从而近似计算这个积分。采样方法有：</p>
<ol style="list-style-type: decimal">
<li><p>概率分布采样，首先求得概率密度的累积密度函数 CDF，然后求得 CDF 的反函数，在0到1之间均匀采样，代入反函数，就得到了采样点。但是实际大部分概率分布不能得到 CDF。</p></li>
<li><p>Rejection Sampling 拒绝采样：对于概率分布 <span class="math inline">\(p(z)\)</span>，引入简单的提议分布 <span class="math inline">\(q(z)\)</span>，使得 <span class="math inline">\(\forall z_i,Mq(z_i)\ge p(z_i)\)</span>。我们先在 $ q(z)$ 中采样，定义接受率：<span class="math inline">\(\alpha=\frac{p(z^i)}{Mq(z^i)}\le1\)</span>。算法描述为：</p>
<ol style="list-style-type: decimal">
<li>取 <span class="math inline">\(z^i\sim q(z)\)</span>。</li>
<li>在均匀分布中选取 <span class="math inline">\(u\)</span>。</li>
<li>如果 <span class="math inline">\(u\le\alpha\)</span>，则接受 <span class="math inline">\(z^i\)</span>，否则，拒绝这个值。</li>
</ol></li>
<li><p>Importance Sampling：直接对期望：<span class="math inline">\(\mathbb{E}_{p(z)}[f(z)]\)</span> 进行采样。
<span class="math display">\[
\mathbb{E}_{p(z)}[f(z)]=\int p(z)f(z)dz=\int \frac{p(z)}{q(z)}f(z)q(z)dz\simeq\frac{1}{N}\sum\limits_{i=1}^Nf(z_i)\frac{p(z_i)}{q(z_i)}
\]</span>
于是采样在 $ q(z)$ 中采样，并通过权重计算和。重要值采样对于权重非常小的时候，效率非常低。重要性采样有一个变种 Sampling-Importance-Resampling，这种方法，首先和上面一样进行采样，然后在采样出来的 <span class="math inline">\(N\)</span> 个样本中，重新采样，这个重新采样，使用每个样本点的权重作为概率分布进行采样。</p></li>
</ol>
</div>
<div id="mcmc" class="section level2 hasAnchor" number="12.2">
<h2><span class="header-section-number">12.2</span> MCMC<a href="马尔可夫链蒙特卡洛.html#mcmc" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>马尔可夫链式一种时间状态都是离散的随机变量序列。我们关注的主要是齐次的一阶马尔可夫链。马尔可夫链满足：<span class="math inline">\(p(X_{t+1}|X_1,X_2,\cdots,X_t)=p(X_{t+1}|X_t)\)</span>。这个式子可以写成转移矩阵的形式 <span class="math inline">\(p_{ij}=p(X_{t+1}=j|X_t=i)\)</span>。我们有：
<span class="math display">\[
\pi_{t+1}(x^*)=\int\pi_i(x)p_{x\to x^*}dx
\]</span>
如果存在 <span class="math inline">\(\pi=(\pi(1),\pi(2),\cdots),\sum\limits_{i=1}^{+\infin}\pi(i)=1\)</span>，有上式成立，这个序列就叫马尔可夫链 <span class="math inline">\(X_t\)</span> 的平稳分布，平稳分布就是表示在某一个时刻后，分布不再改变。MCMC 就是通过构建马尔可夫链概率序列，使其收敛到平稳分布 <span class="math inline">\(p(z)\)</span>。引入细致平衡：<span class="math inline">\(\pi(x)p_{x\to x^*}=\pi(x^*)p_{x^*\to x}\)</span>。如果一个分布满足细致平衡，那么一定满足平稳分布（反之不成立）：
<span class="math display">\[
\int\pi(x)p_{x\to x^*}dx=\int\pi(x^*)p_{x^*\to x}dx=\pi(x^*)
\]</span>
细致平衡条件将平稳分布的序列和马尔可夫链的转移矩阵联系在一起了，通过转移矩阵可以不断生成样本点。假定随机取一个转移矩阵 <span class="math inline">\((Q=Q_{ij})\)</span>，作为一个提议矩阵。我们有：
<span class="math display">\[
p(z)\cdot Q_{z\to z^*}\alpha(z,z^*)=p(z^*)\cdot Q_{z^*\to z}\alpha(z^*,z)
\]</span>
取 ：
<span class="math display">\[
\alpha(z,z^*)=\min\{1,\frac{p(z^*)Q_{z^*\to z}}{p(z)Q_{z\to z^*}}\}
\]</span>
则
<span class="math display">\[
p(z)\cdot Q_{z\to z^*}\alpha(z,z^*)=\min\{p(z)Q_{z\to z^*},p(z^*)Q_{z^*\to z}\}=p(z^*)\cdot Q_{z^*\to z}\alpha(z^*,z)
\]</span>
于是，迭代就得到了序列，这个算法叫做 Metropolis-Hastings 算法：</p>
<ol style="list-style-type: decimal">
<li>通过在0，1之间均匀分布取点 <span class="math inline">\(u\)</span></li>
<li>生成 <span class="math inline">\(z^*\sim Q(z^*|z^{i-1})\)</span></li>
<li>计算 <span class="math inline">\(\alpha\)</span> 值</li>
<li>如果 <span class="math inline">\(\alpha\ge u\)</span>，则 <span class="math inline">\(z^i=z^*\)</span>，否则 <span class="math inline">\(z^{i}=z^{i-1}\)</span></li>
</ol>
<p>这样取的样本就服从 <span class="math inline">\(p(z)=\frac{\hat{p}(z)}{z_p}\sim \hat{p}(z)\)</span>。</p>
<p>下面介绍另一种采样方式 Gibbs 采样，如果 <span class="math inline">\(z\)</span> 的维度非常高，那么通过固定被采样的维度其余的维度来简化采样过程：<span class="math inline">\(z_i\sim p(z_i|z_{-i})\)</span>：</p>
<ol style="list-style-type: decimal">
<li>给定初始值 <span class="math inline">\(z_1^0,z_2^0,\cdots\)</span></li>
<li>在 <span class="math inline">\(t+1\)</span> 时刻，采样 <span class="math inline">\(z_i^{t+1}\sim p(z_i|z_{-i})\)</span>，从第一个维度一个个采样。</li>
</ol>
<p>Gibbs 采样方法是一种特殊的 MH 采样，可以计算 Gibbs 采样的接受率：
<span class="math display">\[
\frac{p(z^*)Q_{z^*\to z}}{p(z)Q_{z\to z^*}}=\frac{p(z_i^*|z^*_{-i})p(z^*_{-i})p(z_i|z_{-i}^*)}{p(z_i|z_{-i})p(z_{-i})p(z_i^*|z_{-i})}
\]</span>
对于每个 Gibbs 采样步骤，<span class="math inline">\(z_{-i}=z_{-i}^*\)</span>，这是由于每个维度 <span class="math inline">\(i\)</span> 采样的时候，其余的参量保持不变。所以上式为1。于是 Gibbs 采样过程中，相当于找到了一个步骤，使得所有的接受率为 1。</p>
</div>
<div id="平稳分布" class="section level2 hasAnchor" number="12.3">
<h2><span class="header-section-number">12.3</span> 平稳分布<a href="马尔可夫链蒙特卡洛.html#平稳分布" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>定义随机矩阵：
<span class="math display">\[
Q=\begin{pmatrix}Q_{11}&amp;Q_{12}&amp;\cdots&amp;Q_{1K}\\\vdots&amp;\vdots&amp;\vdots&amp;\vdots\\Q_{k1}&amp;Q_{k2}&amp;\cdots&amp;Q_{KK}\end{pmatrix}
\]</span>
这个矩阵每一行或者每一列的和都是1。随机矩阵的特征值都小于等于1。假设只有一个特征值为 <span class="math inline">\(\lambda_i=1\)</span>。于是在马尔可夫过程中：
<span class="math display">\[
q^{t+1}(x=j)=\sum\limits_{i=1}^Kq^t(x=i)Q_{ij}\\
\Rightarrow q^{t+1}=q^t\cdot Q=q^1Q^t
\]</span>
于是有：
<span class="math display">\[
q^{t+1}=q^1A\Lambda^t A^{-1}
\]</span>
如果 <span class="math inline">\(m\)</span> 足够大，那么，<span class="math inline">\(\Lambda^m=diag(0,0,\cdots,1,\cdots,0)\)</span>，则：<span class="math inline">\(q^{m+1}=q^{m}\)</span> ，则趋于平稳分布了。马尔可夫链可能具有平稳分布的性质，所以我们可以构建马尔可夫链使其平稳分布收敛于需要的概率分布（设计转移矩阵）。</p>
<p>在采样过程中，需要经历一定的时间（燃烧期/混合时间）才能达到平稳分布。但是 MCMC 方法有一些问题：</p>
<ol style="list-style-type: decimal">
<li>无法判断是否已经收敛</li>
<li>燃烧期过长（维度太高，并且维度之间有关，可能无法采样到某些维度），例如在 GMM 中，可能无法采样到某些峰。于是在一些模型中，需要对隐变量之间的关系作出约束，如 RBM 假设隐变量之间无关。</li>
<li>样本之间一定是有相关性的，如果每个时刻都取一个点，那么每个样本一定和前一个相关，这可以通过间隔一段时间采样。</li>
</ol>
</div>
<div id="隐马尔可夫模型" class="section level2 hasAnchor" number="12.4">
<h2><span class="header-section-number">12.4</span> 隐马尔可夫模型<a href="马尔可夫链蒙特卡洛.html#隐马尔可夫模型" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>隐马尔可夫模型是一种概率图模型。我们知道，机器学习模型可以从频率派和贝叶斯派两个方向考虑，在频率派的方法中的核心是优化问题，而在贝叶斯派的方法中，核心是积分问题，也发展出来了一系列的积分方法如变分推断，MCMC 等。概率图模型最基本的模型可以分为有向图（贝叶斯网络）和无向图（马尔可夫随机场）两个方面，例如 GMM，在这些基本的模型上，如果样本之间存在关联，可以认为样本中附带了时序信息，从而样本之间不独立全同分布的，这种模型就叫做动态模型，隐变量随着时间发生变化，于是观测变量也发生变化：</p>
<pre class="mermaid"><code>graph LR;
    z1--&gt;z2--&gt;z3;
    </code></pre>
<p>根据状态变量的特点，可以分为：</p>
<ol style="list-style-type: decimal">
<li>HMM，状态变量（隐变量）是离散的</li>
<li>Kalman 滤波，状态变量是连续的，线性的</li>
<li>粒子滤波，状态变量是连续，非线性的</li>
</ol>
</div>
<div id="hmm" class="section level2 hasAnchor" number="12.5">
<h2><span class="header-section-number">12.5</span> HMM<a href="马尔可夫链蒙特卡洛.html#hmm" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>HMM 用概率图表示为：</p>
<pre class="mermaid"><code>graph TD;
t1--&gt;t2;
subgraph four
    t4--&gt;x4((x4))
end
subgraph three
    t3--&gt;x3((x3))
end
subgraph two
    t2--&gt;x2((x2))
end
subgraph one
    t1--&gt;x1((x1))
end

t2--&gt;t3;
t3--&gt;t4;</code></pre>
<p>上图表示了四个时刻的隐变量变化。用参数 <span class="math inline">\(\lambda=(\pi,A,B)\)</span> 来表示，其中 <span class="math inline">\(\pi\)</span> 是开始的概率分布，<span class="math inline">\(A\)</span> 为状态转移矩阵，<span class="math inline">\(B\)</span> 为发射矩阵。</p>
<p>下面使用 $ o_t$ 来表示观测变量，<span class="math inline">\(O\)</span> 为观测序列，<span class="math inline">\(V=\{v_1,v_2,\cdots,v_M\}\)</span> 表示观测的值域，<span class="math inline">\(i_t\)</span> 表示状态变量，<span class="math inline">\(I\)</span> 为状态序列，<span class="math inline">\(Q=\{q_1,q_2,\cdots,q_N\}\)</span> 表示状态变量的值域。定义 <span class="math inline">\(A=(a_{ij}=p(i_{t+1}=q_j|i_t=q_i))\)</span> 表示状态转移矩阵，<span class="math inline">\(B=(b_j(k)=p(o_t=v_k|i_t=q_j))\)</span> 表示发射矩阵。</p>
<p>在 HMM 中，有两个基本假设：</p>
<ol style="list-style-type: decimal">
<li><p>齐次 Markov 假设（未来只依赖于当前）：
<span class="math display">\[
p(i_{t+1}|i_t,i_{t-1},\cdots,i_1,o_t,o_{t-1},\cdots,o_1)=p(i_{t+1}|i_t)
\]</span></p></li>
<li><p>观测独立假设：
<span class="math display">\[
p(o_t|i_t,i_{t-1},\cdots,i_1,o_{t-1},\cdots,o_1)=p(o_t|i_t)
\]</span></p></li>
</ol>
<p>HMM 要解决三个问题：</p>
<ol style="list-style-type: decimal">
<li>Evaluation：<span class="math inline">\(p(O|\lambda)\)</span>，Forward-Backward 算法</li>
<li>Learning：<span class="math inline">\(\lambda=\mathop{argmax}\limits_{\lambda}p(O|\lambda)\)</span>，EM 算法（Baum-Welch）</li>
<li>Decoding：<span class="math inline">\(I=\mathop{argmax}\limits_{I}p(I|O,\lambda)\)</span>，Vierbi 算法
<ol style="list-style-type: decimal">
<li>预测问题：<span class="math inline">\(p(i_{t+1}|o_1,o_2,\cdots,o_t)\)</span></li>
<li>滤波问题：<span class="math inline">\(p(i_t|o_1,o_2,\cdots,o_t)\)</span></li>
</ol></li>
</ol>
<div id="evaluation" class="section level3 hasAnchor" number="12.5.1">
<h3><span class="header-section-number">12.5.1</span> Evaluation<a href="马尔可夫链蒙特卡洛.html#evaluation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><span class="math display">\[
p(O|\lambda)=\sum\limits_{I}p(I,O|\lambda)=\sum\limits_{I}p(O|I,\lambda)p(I|\lambda)
\]</span></p>
<p><span class="math display">\[
p(I|\lambda)=p(i_1,i_2,\cdots,i_t|\lambda)=p(i_t|i_1,i_2,\cdots,i_{t-1},\lambda)p(i_1,i_2,\cdots,i_{t-1}|\lambda)
\]</span></p>
<p>根据齐次 Markov 假设：
<span class="math display">\[
p(i_t|i_1,i_2,\cdots,i_{t-1},\lambda)=p(i_t|i_{t-1})=a_{i_{t-1}i_t}
\]</span>
所以：
<span class="math display">\[
p(I|\lambda)=\pi_1\prod\limits_{t=2}^Ta_{i_{t-1},i_t}
\]</span>
又由于：
<span class="math display">\[
p(O|I,\lambda)=\prod\limits_{t=1}^Tb_{i_t}(o_t)
\]</span>
于是：
<span class="math display">\[
p(O|\lambda)=\sum\limits_{I}\pi_{i_1}\prod\limits_{t=2}^Ta_{i_{t-1},i_t}\prod\limits_{t=1}^Tb_{i_t}(o_t)
\]</span>
我们看到，上面的式子中的求和符号是对所有的观测变量求和，于是复杂度为 <span class="math inline">\(O(N^T)\)</span>。</p>
<p>下面，记 <span class="math inline">\(\alpha_t(i)=p(o_1,o_2,\cdots,o_t,i_t=q_i|\lambda)\)</span>，所以，<span class="math inline">\(\alpha_T(i)=p(O,i_T=q_i|\lambda)\)</span>。我们看到：
<span class="math display">\[
p(O|\lambda)=\sum\limits_{i=1}^Np(O,i_T=q_i|\lambda)=\sum\limits_{i=1}^N\alpha_T(i)
\]</span>
对 <span class="math inline">\(\alpha_{t+1}(j)\)</span>：
<span class="math display">\[
\begin{align}\alpha_{t+1}(j)&amp;=p(o_1,o_2,\cdots,o_{t+1},i_{t+1}=q_j|\lambda)\nonumber\\
&amp;=\sum\limits_{i=1}^Np(o_1,o_2,\cdots,o_{t+1},i_{t+1}=q_j,i_t=q_i|\lambda)\nonumber\\
&amp;=\sum\limits_{i=1}^Np(o_{t+1}|o_1,o_2,\cdots,i_{t+1}=q_j,i_t=q_i|\lambda)p(o_1,\cdots,o_t,i_t=q_i,i_{t+1}=q_j|\lambda)
\end{align}
\]</span>
利用观测独立假设：
<span class="math display">\[
\begin{align}\alpha_{t+1}(j)&amp;=\sum\limits_{i=1}^Np(o_{t+1}|i_{t+1}=q_j)p(o_1,\cdots,o_t,i_t=q_i,i_{t+1}=q_j|\lambda)\nonumber\\
&amp;=\sum\limits_{i=1}^Np(o_{t+1}|i_{t+1}=q_j)p(i_{t+1}=q_j|o_1,\cdots,o_t,i_t=q_i,\lambda)p(o_1,\cdots,o_t,i_t=q_i|\lambda)\nonumber\\
&amp;=\sum\limits_{i=1}^Nb_{j}(o_t)a_{ij}\alpha_t(i)
\end{align}
\]</span>
上面利用了齐次 Markov 假设得到了一个递推公式，这个算法叫做前向算法。</p>
<p>还有一种算法叫做后向算法，定义 <span class="math inline">\(\beta_t(i)=p(o_{t+1},o_{t+1},\cdots，o_T|i_t=i,\lambda)\)</span>：
<span class="math display">\[
\begin{align}p(O|\lambda)&amp;=p(o_1,\cdots,o_T|\lambda)\nonumber\\
&amp;=\sum\limits_{i=1}^Np(o_1,o_2,\cdots,o_T,i_1=q_i|\lambda)\nonumber\\
&amp;=\sum\limits_{i=1}^Np(o_1,o_2,\cdots,o_T|i_1=q_i,\lambda)\pi_i\nonumber\\
&amp;=\sum\limits_{i=1}^Np(o_1|o_2,\cdots,o_T,i_1=q_i,\lambda)p(o_2,\cdots,o_T|i_1=q_i,\lambda)\pi_i\nonumber\\
&amp;=\sum\limits_{i=1}^Nb_i(o_1)\pi_i\beta_1(i)
\end{align}
\]</span>
对于这个 <span class="math inline">\(\beta_1(i)\)</span>：
<span class="math display">\[
\begin{align}\beta_t(i)&amp;=p(o_{t+1},\cdots,o_T|i_t=q_i)\nonumber\\
&amp;=\sum\limits_{j=1}^Np(o_{t+1},o_{t+2},\cdots,o_T,i_{t+1}=q_j|i_t=q_i)\nonumber\\
&amp;=\sum\limits_{j=1}^Np(o_{t+1},\cdots,o_T|i_{t+1}=q_j,i_t=q_i)p(i_{t+1}=q_j|i_t=q_i)\nonumber\\
&amp;=\sum\limits_{j=1}^Np(o_{t+1},\cdots,o_T|i_{t+1}=q_j)a_{ij}\nonumber\\
&amp;=\sum\limits_{j=1}^Np(o_{t+1}|o_{t+2},\cdots,o_T,i_{t+1}=q_j)p(o_{t+2},\cdots,o_T|i_{t+1}=q_j)a_{ij}\nonumber\\
&amp;=\sum\limits_{j=1}^Nb_j(o_{t+1})a_{ij}\beta_{t+1}(j)
\end{align}
\]</span>
于是后向地得到了第一项。</p>
</div>
<div id="learning" class="section level3 hasAnchor" number="12.5.2">
<h3><span class="header-section-number">12.5.2</span> Learning<a href="马尔可夫链蒙特卡洛.html#learning" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>为了学习得到参数的最优值，在 MLE 中：
<span class="math display">\[
\lambda_{MLE}=\mathop{argmax}_\lambda p(O|\lambda)
\]</span>
我们采用 EM 算法（在这里也叫 Baum Welch 算法），用上标表示迭代：
<span class="math display">\[
\theta^{t+1}=\mathop{argmax}_{\theta}\int_z\log p(X,Z|\theta)p(Z|X,\theta^t)dz
\]</span>
其中，<span class="math inline">\(X\)</span> 是观测变量，<span class="math inline">\(Z\)</span> 是隐变量序列。于是：
<span class="math display">\[
\lambda^{t+1}=\mathop{argmax}_\lambda\sum\limits_I\log p(O,I|\lambda)p(I|O,\lambda^t)\\
=\mathop{argmax}_\lambda\sum\limits_I\log p(O,I|\lambda)p(O,I|\lambda^t)
\]</span>
这里利用了 <span class="math inline">\(p(O|\lambda^t)\)</span> 和<span class="math inline">\(\lambda\)</span> 无关。将 Evaluation 中的式子代入：
<span class="math display">\[
\sum\limits_I\log p(O,I|\lambda)p(O,I|\lambda^t)=\sum\limits_I[\log \pi_{i_1}+\sum\limits_{t=2}^T\log a_{i_{t-1},i_t}+\sum\limits_{t=1}^T\log b_{i_t}(o_t)]p(O,I|\lambda^t)
\]</span>
对 <span class="math inline">\(\pi^{t+1}\)</span>：
<span class="math display">\[
\begin{align}\pi^{t+1}&amp;=\mathop{argmax}_\pi\sum\limits_I[\log \pi_{i_1}p(O,I|\lambda^t)]\nonumber\\
&amp;=\mathop{argmax}_\pi\sum\limits_I[\log \pi_{i_1}\cdot p(O,i_1,i_2,\cdots,i_T|\lambda^t)]
\end{align}
\]</span>
上面的式子中，对 <span class="math inline">\(i_2,i_2,\cdots,i_T\)</span> 求和可以将这些参数消掉：
<span class="math display">\[
\pi^{t+1}=\mathop{argmax}_\pi\sum\limits_{i_1}[\log \pi_{i_1}\cdot p(O,i_1|\lambda^t)]
\]</span>
上面的式子还有对 <span class="math inline">\(\pi\)</span> 的约束 <span class="math inline">\(\sum\limits_i\pi_i=1\)</span>。定义 Lagrange 函数：
<span class="math display">\[
L(\pi,\eta)=\sum\limits_{i=1}^N\log \pi_i\cdot p(O,i_1=q_i|\lambda^t)+\eta(\sum\limits_{i=1}^N\pi_i-1)
\]</span>
于是：
<span class="math display">\[
\frac{\partial L}{\partial\pi_i}=\frac{1}{\pi_i}p(O,i_1=q_i|\lambda^t)+\eta=0
\]</span>
对上式求和：
<span class="math display">\[
\sum\limits_{i=1}^Np(O,i_1=q_i|\lambda^t)+\pi_i\eta=0\Rightarrow\eta=-p(O|\lambda^t)
\]</span>
所以：
<span class="math display">\[
\pi_i^{t+1}=\frac{p(O,i_1=q_i|\lambda^t)}{p(O|\lambda^t)}
\]</span></p>
</div>
<div id="decoding" class="section level3 hasAnchor" number="12.5.3">
<h3><span class="header-section-number">12.5.3</span> Decoding<a href="马尔可夫链蒙特卡洛.html#decoding" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Decoding 问题表述为：
<span class="math display">\[
I=\mathop{argmax}\limits_{I}p(I|O,\lambda)
\]</span>
我们需要找到一个序列，其概率最大，这个序列就是在参数空间中的一个路径，可以采用动态规划的思想。</p>
<p>定义：
<span class="math display">\[
\delta_{t}(j)=\max\limits_{i_1,\cdots,i_{t-1}}p(o_1,\cdots,o_t,i_1,\cdots,i_{t-1},i_t=q_i)
\]</span>
于是：
<span class="math display">\[
\delta_{t+1}(j)=\max\limits_{1\le i\le N}\delta_t(i)a_{ij}b_j(o_{t+1})
\]</span>
这个式子就是从上一步到下一步的概率再求最大值。记这个路径为：
<span class="math display">\[
\psi_{t+1}(j)=\mathop{argmax}\limits_{1\le i\le N}\delta_t(i)a_{ij}
\]</span></p>
</div>
</div>
<div id="小结-5" class="section level2 hasAnchor" number="12.6">
<h2><span class="header-section-number">12.6</span> 小结<a href="马尔可夫链蒙特卡洛.html#小结-5" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>HMM 是一种动态模型，是由混合树形模型和时序结合起来的一种模型（类似 GMM + Time）。对于类似 HMM 的这种状态空间模型，普遍的除了学习任务（采用 EM ）外，还有推断任务，推断任务包括：</p>
<ol style="list-style-type: decimal">
<li><p>译码 Decoding：<span class="math inline">\(p(z_1,z_2,\cdots,z_t|x_1,x_2,\cdots,x_t)\)</span></p></li>
<li><p>似然概率：<span class="math inline">\(p(X|\theta)\)</span></p></li>
<li><p>滤波：<span class="math inline">\(p(z_t|x_1,\cdots,x_t)\)</span>，Online
<span class="math display">\[
p(z_t|x_{1:t})=\frac{p(x_{1:t},z_t)}{p(x_{1:t})}=C\alpha_t(z_t)
\]</span></p></li>
<li><p>平滑：<span class="math inline">\(p(z_t|x_1,\cdots,x_T)\)</span>，Offline
<span class="math display">\[
p(z_t|x_{1:T})=\frac{p(x_{1:T},z_t)}{p(x_{1:T})}=\frac{\alpha_t(z_t)p(x_{t+1:T}|x_{1:t},z_t)}{p(x_{1:T})}
\]</span>
根据概率图的条件独立性，有：
<span class="math display">\[
p(z_t|x_{1:T})=\frac{\alpha_t(z_t)p(x_{t+1:T}|z_t)}{p(x_{1:T})}=C\alpha_t(z_t)\beta_t(z_t)
\]</span>
这个算法叫做前向后向算法。</p></li>
<li><p>预测：<span class="math inline">\(p(z_{t+1},z_{t+2}|x_1,\cdots,x_t),p(x_{t+1},x_{t+2}|x_1,\cdots,x_t)\)</span>
<span class="math display">\[
p(z_{t+1}|x_{1:t})=\sum_{z_t}p(z_{t+1},z_t|x_{1:t})=\sum\limits_{z_t}p(z_{t+1}|z_t)p(z_t|x_{1:t})
\]</span></p>
<p><span class="math display">\[
p(x_{t+1}|x_{1:t})=\sum\limits_{z_{t+1}}p(x_{t+1},z_{t+1}|x_{1:t})=\sum\limits_{z_{t+1}}p(x_{t+1}|z_{t+1})p(z_{t+1}|x_{1:t})
\]</span></p></li>
</ol>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="变分推断.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="线性动态系统.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["CBook.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
